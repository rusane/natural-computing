{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Training D1_D2 dataset using the `Stacking` ensemble method.\n",
    "\n",
    "### Author\n",
    "Ajinkya Indulkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# data loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data pre-processing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyts.preprocessing import InterpolationImputer\n",
    "\n",
    "# model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# set seed (for reproducibility)\n",
    "np.random.seed(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    \"\"\"\n",
    "    @description: clean data (imputation by interpolation \n",
    "    used to handle missing data)\n",
    "    @arguments:\n",
    "        df (pd.Dataframe) - raw dataset\n",
    "    @returns:\n",
    "        df (pd.Dataframe) - cleaned dataset\n",
    "    \"\"\"\n",
    "    # list of columns for training (starting point)\n",
    "    getting_started = [\"RID\", \"VISCODE\", \"DX_bl\", \"DX\", \"ADAS13\", \"Ventricles\", \"CDRSB\", \"ADAS11\", \"MMSE\", \n",
    "                       \"RAVLT_immediate\", \"Hippocampus\", \"WholeBrain\", \"Entorhinal\", \"MidTemp\", \"FDG\", \"AV45\",\n",
    "                       \"ABETA_UPENNBIOMK9_04_19_17\", \"TAU_UPENNBIOMK9_04_19_17\", \"PTAU_UPENNBIOMK9_04_19_17\", \"APOE4\", \n",
    "                       \"AGE\"]\n",
    "    # filter dataframe based on list of columns selected\n",
    "    df = df[getting_started]\n",
    "    \n",
    "    # filter out only baseline visits\n",
    "    df = df[df[\"VISCODE\"] == \"bl\"].reset_index(drop=True).drop(columns='VISCODE')\n",
    "    \n",
    "    # fix \"DX\" column\n",
    "    df['DX'] = df['DX'].fillna('nan')\n",
    "    \n",
    "    # remove rows with nan DX values\n",
    "    df = df[df['DX'] != 'nan'].reset_index(drop=True)\n",
    "    \n",
    "    # fix \"ABETA_UPENNBIOMK9_04_19_17, TAU_UPENNBIOMK9_04_19_17, PTAU_UPENNBIOMK9_04_19_17\" column\n",
    "    err_cols = [\"ABETA_UPENNBIOMK9_04_19_17\", \"TAU_UPENNBIOMK9_04_19_17\", \"PTAU_UPENNBIOMK9_04_19_17\"]\n",
    "    for c in err_cols:\n",
    "        df[c] = df[c].apply(lambda x: None if x == ' ' or '<' in x or '>' in x else x)\n",
    "        df[c] = df[c].astype(float)\n",
    "    \n",
    "    # convert \"DX_bl\" and \"DX\" to categorical values\n",
    "    df[['DX_bl', 'DX']] = df[['DX_bl', 'DX']].apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    # handle missing data (imputation by interpolation)\n",
    "    impcols = [c for c in df.columns.tolist() if c not in ['RID', 'DX_bl']]\n",
    "    features = df.drop(columns=['RID', 'DX_bl']).to_numpy()\n",
    "    features = InterpolationImputer().fit_transform(features)\n",
    "    df[impcols] = pd.DataFrame(features)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading + Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"./tadpole_challenge/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv\n",
    "d1_d2_df = pd.read_csv(ROOT+\"TADPOLE_D1_D2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataset\n",
    "train_df = clean_dataset(d1_d2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['RID', 'DX_bl']).to_numpy()\n",
    "y = train_df['DX_bl'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Decision Tree model (base learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Classifier accuracy: 0.7890173410404624\n"
     ]
    }
   ],
   "source": [
    "print('DT Classifier accuracy:', dtc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Stacking model with DT as base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('svr', make_pipeline(StandardScaler(),\n",
    "                              LinearSVC(random_state=42)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stk_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=None,\n",
       "                   estimators=[('dt',\n",
       "                                DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                       class_weight=None,\n",
       "                                                       criterion='gini',\n",
       "                                                       max_depth=None,\n",
       "                                                       max_features=None,\n",
       "                                                       max_leaf_nodes=None,\n",
       "                                                       min_impurity_decrease=0.0,\n",
       "                                                       min_impurity_split=None,\n",
       "                                                       min_samples_leaf=1,\n",
       "                                                       min_samples_split=2,\n",
       "                                                       min_weight_fraction_leaf=0.0,\n",
       "                                                       presort='deprecated',\n",
       "                                                       random_state=None,\n",
       "                                                       splitter='best')),\n",
       "                               ('svr...\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0))],\n",
       "                                         verbose=False))],\n",
       "                   final_estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                      dual=False,\n",
       "                                                      fit_intercept=True,\n",
       "                                                      intercept_scaling=1,\n",
       "                                                      l1_ratio=None,\n",
       "                                                      max_iter=100,\n",
       "                                                      multi_class='auto',\n",
       "                                                      n_jobs=None, penalty='l2',\n",
       "                                                      random_state=None,\n",
       "                                                      solver='lbfgs',\n",
       "                                                      tol=0.0001, verbose=0,\n",
       "                                                      warm_start=False),\n",
       "                   n_jobs=None, passthrough=False, stack_method='auto',\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stk_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier accuracy: 0.8208092485549133\n"
     ]
    }
   ],
   "source": [
    "print('Stacking Classifier accuracy:', stk_clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
